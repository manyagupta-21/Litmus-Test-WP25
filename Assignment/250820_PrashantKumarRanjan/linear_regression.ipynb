{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **Linear Regression**\n",
        "We will use Linear regression for predicting house prices\n",
        "\n",
        "We are using a Kaggle dataset- https://www.kaggle.com/harlfoxem/housesalesprediction"
      ],
      "metadata": {
        "id": "FSRalact4xj4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lets import required Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n"
      ],
      "metadata": {
        "id": "J8MRCcJY2btt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Dataset Preparation**"
      ],
      "metadata": {
        "id": "1MzAhMmE6UHE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Execute this cell for loading dataset in a pandas dataframe\n",
        "\n",
        "from IPython.display import clear_output\n",
        "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=16x6-8Znn2T50zFwVvKlzsdN7Jd1hpjct' -O Linear_regression_dataset\n",
        "\n",
        "data_df = pd.read_csv(\"Linear_regression_dataset\")"
      ],
      "metadata": {
        "id": "LiWI-2py554R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lets have a quick Look at dataset\n",
        "\n",
        "print(\"(No of rows, No of Columns) = \",data_df.shape)\n",
        "data_df.head()"
      ],
      "metadata": {
        "id": "TAodxbYX7AKf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So there are **19** features (of course we will not use id as feature :) ), and 1 variable to predict(price)\n",
        "\n",
        "But note that the **date** column contain strings so first we will remove T00.. part from it and than convert it to numpy array."
      ],
      "metadata": {
        "id": "gsJaooGZ7pUV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_df['date'] = data_df[\"date\"].str.replace(\"T000000\", \"\", regex=False).astype(int)                                         # Remove T000000 part from data column. Hint: search about .str.replace() method. :)\n",
        "\n",
        "data_array = data_df.drop(columns=[\"id\"]).to_numpy()                                             # Create a numpy array which does not have \"id\" field\n",
        "assert (data_array.shape == (21613,20))\n",
        "\n",
        "data_df.head()"
      ],
      "metadata": {
        "id": "FNFNf3jT7oxW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now the next task is **normalization**.\n",
        "\n",
        "We will scale each column of dataset by x -> (x-u)/s\n",
        "\n",
        "where u is mean(x), and s is standard deviation of u"
      ],
      "metadata": {
        "id": "xsBZxZ4x-oBR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mean = data_array.mean(axis=0)                                  # this should be an array, each entry should be mean of a column\n",
        "sd = data_array.std(axis=0)                                    # this should be an array, each entry should be standard deviation of a column\n",
        "\n",
        "data_array_norm = (data_array-mean)/sd\n",
        "\n",
        "print(data_array_norm.shape)"
      ],
      "metadata": {
        "id": "u7GZV-0T_zCy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The last step is to make train and test dataset and to create seperate vector for price"
      ],
      "metadata": {
        "id": "VCQTrNIgAlPv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "labels = data_array_norm[:, 1]                                                                                                            # extract the price column from data\n",
        "x_array_norm = np.delete(data_array_norm, 1, axis=1)                                                                                                      # delete the price column from data_array_norm. Hint: use np.delete()\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(x_array_norm,labels,test_size=0.15,random_state=42,shuffle=True)    # splitting data into test and train set.\n",
        "\n",
        "print(x_train.shape,x_test.shape,y_train.shape,y_test.shape)"
      ],
      "metadata": {
        "id": "dJyX5QOFBRg5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Loss and gradient descent**\n",
        "We will use mean squared error(MSE) as loss\n",
        "\n",
        "Use the gradient descent algorithm which you learned from tutorials\n",
        "\n",
        "Your task is to complete the following functions"
      ],
      "metadata": {
        "id": "iAdW-22ZDcdU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def loss(y_pred,y_true):\n",
        "  \"\"\"\n",
        "  input:\n",
        "  y_pred = [array] predicted value of y\n",
        "  y_true = [array] ground truth\n",
        "\n",
        "  output:\n",
        "  mse: [scalar] the MES loss\n",
        "  \"\"\"\n",
        "  mse = np.mean((y_pred - y_true) ** 2)                      # fill code here\n",
        "\n",
        "  return mse"
      ],
      "metadata": {
        "id": "ufoIQOpeEFQx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def y(x,a,b):\n",
        "  \"\"\"\n",
        "  This function should return predicted value of y = ax+b\n",
        "  input:\n",
        "  x: [array] the feature vector of shape (m,n)\n",
        "  a: [array] weights of shape (n,)\n",
        "  b: [scalar] bias\n",
        "\n",
        "  output:\n",
        "  y_pred: [array] predicted value of y of shape (m,)\n",
        "  \"\"\"\n",
        "\n",
        "  m,n = x.shape\n",
        "  y_pred = x@a+b                   # fill code here\n",
        "\n",
        "  assert(y_pred.shape==(m,))\n",
        "  return y_pred"
      ],
      "metadata": {
        "id": "a6LogNz5E28X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient(x,a,b,y_true):\n",
        "  \"\"\"\n",
        "  This function shoud return gradient of loss\n",
        "  input:\n",
        "  x: [array] the feature vector of shape (m,n)\n",
        "  a: [array] weights of shape (n,)\n",
        "  b: [scalar] bias\n",
        "  y_true: [array] ground truth of shape (m,)\n",
        "\n",
        "  output:\n",
        "  grad: [tuple] a tuple (derivative with respect to a[array of shape(n,)], derivative with respect to b[scalar])\n",
        "  \"\"\"\n",
        "  m,n = x.shape\n",
        "  yp = y(x,a,b)\n",
        "\n",
        "  da = (2/m)*(x.T@(yp-y_true))              # write code to calculate derivative of loss with respect to a\n",
        "  db = (2/m)*np.sum((yp-y_true))              # write code to calculate derivative of loss with respect to b\n",
        "\n",
        "  assert(da.shape ==(n,))\n",
        "  return (da,db)"
      ],
      "metadata": {
        "id": "lYnPROu8Gxwi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient_descent(x,y_true,learning_rate=0.01,epochs = 10):\n",
        "  \"\"\"\n",
        "  This function perfroms gradient descent and minimizes loss\n",
        "  input:\n",
        "  x: [array] the feature vector of shape (m,n)\n",
        "  y_true: [array] ground truth of shape (m,)\n",
        "\n",
        "  output:\n",
        "  loss: [array] of size (epochs,)\n",
        "  weights: [tuple] (a,b)\n",
        "  \"\"\"\n",
        "  m,n = x.shape\n",
        "  loss_mse = []                                 # initialize empty list to store loss\n",
        "  a = np.zeros(n)                                       # initialize a- weights and b- bias\n",
        "  b = 0\n",
        "\n",
        "  for i in range(epochs):\n",
        "    # calculate derivative using gradient() function\n",
        "    da, db = gradient(x, a, b, y_true)\n",
        "    # apply gradient descent now to update a and b\n",
        "    a = a - learning_rate * da\n",
        "    b = b - learning_rate * db\n",
        "\n",
        "    l_mse = loss(y(x, a, b), y_true)                                # calculate loss at this point\n",
        "    loss_mse.append(l_mse)\n",
        "\n",
        "    print(\"Epoch \",i+1,\" Completed!\",\"loss = \",l_mse)\n",
        "\n",
        "  print(\"Training completed!!\")\n",
        "\n",
        "  assert(a.shape==(n,))\n",
        "\n",
        "  return (loss_mse,a,b)"
      ],
      "metadata": {
        "id": "km_z3ojKKQdj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Training**"
      ],
      "metadata": {
        "id": "VsR5XLl_WVu4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 500              # tweak this!!!\n",
        "learn_rate = 0.02          # choose learning rate wisely otherwise loss may diverge!!\n",
        "\n",
        "train_loss,a,b = gradient_descent(x_train, y_train, learning_rate=learn_rate, epochs=epochs)"
      ],
      "metadata": {
        "id": "5A9mqkqLWU27"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Evaluation and Visualization**\n",
        "Lets plot how loss varies with epochs\n"
      ],
      "metadata": {
        "id": "TDH-7NHQT50f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss = loss(y(x_test, a, b), y_test)\n",
        "\n",
        "print(\"Loss on test data = \",test_loss)\n",
        "\n",
        "# Visualization of loss\n",
        "\n",
        "plt.plot(train_loss)                   # plot loss versus epochs\n",
        "plt.title(\"Training Loss\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "d7JRB_nJUEkV"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
    },
    "kernelspec": {
      "display_name": "Python 3.8.10 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}